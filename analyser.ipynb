{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP: Assignment 2\n",
    "### Group 11: Teo Stereciu (s4678826) & Csanad Vegh (s4739124)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Part II of the second assignment, we designed two feedforward neural networks to classify movie reviews into positive and negative. The first one takes as input a vector embedding based on TF-IDF and the second one uses Word2Vec embeddings. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "\n",
    "Here we set up all dependencies for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the data from a local source. The reviews have already been split threeway, into training, validation, and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset into memory\n",
    "def load_data (filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    corpus = df[\"text\"]\n",
    "    target = df[\"label\"]\n",
    "    return corpus, target\n",
    "\n",
    "corpus_valid, target_valid = load_data(\"IMDB/Valid.csv\")\n",
    "corpus_train, target_train = load_data(\"IMDB/Train.csv\")\n",
    "corpus_test, target_test = load_data(\"IMDB/Test.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look into our training set. Our analysis points out that there are two possible labels and that the training corpus is decently sized and balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible sentiments are [0 1]\n",
      "The number of reviews for training is 40000\n",
      "Training corpus is 49% positive reviews\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw text example</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A terrible movie as everyone has said. What ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    raw text example\n",
       "0  A terrible movie as everyone has said. What ma..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Possible sentiments are\", np.unique(target_train))\n",
    "print(\"The number of reviews for training is\", len(corpus_train))\n",
    "size = len(corpus_train) + len(corpus_valid) + len(corpus_test)\n",
    "print(\"Training corpus is \" + str(int(100*np.sum(target_train)/len(target_train))) + \"% positive reviews\")\n",
    "info = pd.DataFrame([corpus_train[5]], columns=[\"raw text example\"]) # use to track progress\n",
    "info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to clean up the data. We want to obtain clean tokens, but put them back together into a string for practical reasons that will become apparent soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# helper function to convert the pos tag format into something compatible with the lemmatizer\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# turn the dataset into clean tokens\n",
    "def clean_data(doc):\n",
    "    doc = re.sub(r'<[^>]+>', '', doc)  # remove HTML tags\n",
    "    doc = re.sub(r'\\W+', ' ', doc) # remove every char that is not alphanumeric, keep spaces\n",
    "    tokens = wordpunct_tokenize(doc) \n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stop_words] \n",
    "    pos = pos_tag(tokens)\n",
    "    clean_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in pos]\n",
    "\n",
    "    # put tokens back into string format for tfidf vectorizer\n",
    "    clean_doc = \" \".join(clean_tokens)\n",
    "\n",
    "    return clean_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = target_train\n",
    "for line in corpus_train:\n",
    "    clean_line = clean_data(line)\n",
    "    X_train.append(clean_line)\n",
    "\n",
    "X_valid = []\n",
    "y_valid = target_valid\n",
    "for line in corpus_valid:\n",
    "    clean_line = clean_data(line)\n",
    "    X_valid.append(clean_line)\n",
    "\n",
    "X_test = []\n",
    "y_test = target_test\n",
    "for line in corpus_test:\n",
    "    clean_line = clean_data(line)\n",
    "    X_test.append(clean_line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what the clean text looks like compared to the original. We also find how many (unique) tokens there are in the training corpus to get a sense of how complex our models have to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw text example</th>\n",
       "      <th>clean text example</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>num tokens in corpus</th>\n",
       "      <th>num unique tokens in corpus</th>\n",
       "      <th>avg review length</th>\n",
       "      <th>max review length</th>\n",
       "      <th>min review length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A terrible movie as everyone has said. What ma...</td>\n",
       "      <td>terrible movie everyone say make laugh cameo a...</td>\n",
       "      <td>0</td>\n",
       "      <td>4786521</td>\n",
       "      <td>84468</td>\n",
       "      <td>119.663025</td>\n",
       "      <td>1429</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    raw text example  \\\n",
       "0  A terrible movie as everyone has said. What ma...   \n",
       "\n",
       "                                  clean text example  sentiment  \\\n",
       "0  terrible movie everyone say make laugh cameo a...          0   \n",
       "\n",
       "   num tokens in corpus  num unique tokens in corpus  avg review length  \\\n",
       "0               4786521                        84468         119.663025   \n",
       "\n",
       "   max review length  min review length  \n",
       "0               1429                  3  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info[\"clean text example\"] = X_train[5]\n",
    "info[\"sentiment\"] = y_train[5]\n",
    "review_len = [sum(1 for word in review.split()) for review in X_train]\n",
    "tokens = [[word for word in review.split()] for review in X_train]\n",
    "flat_tokens = [token for review in tokens for token in review]\n",
    "num_tokens_unique = len(set(flat_tokens))\n",
    "info[\"num tokens in corpus\"] = np.sum(review_len)\n",
    "info[\"num unique tokens in corpus\"] = num_tokens_unique\n",
    "info[\"avg review length\"] = np.mean(review_len)\n",
    "info[\"max review length\"] = np.max(review_len)\n",
    "info[\"min review length\"] = np.min(review_len)\n",
    "info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll be focusing on the TF-IDF method. In a nutshell, we'll be using a weigthed sum to represent the words in vector space. The TF-IDF score give more importance to words that not only have high occurence in an arbitrary document, but also occur in many documents in the corpus. We also choose to ignore rare terms (i.e., that appear in less than 10% of the reviews) because highly movie-specific words would not be useful during training. Note that the sklearn.feature_extraction.text.TfidfVectorizer() takes as input text rather than tokens, hence why we put the tokens back together earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model_tfidf(size):\n",
    "    model_tfidf = Pipeline([\n",
    "        (\"vect\", TfidfVectorizer(min_df=0.1)), \n",
    "        (\"clf\", MLPClassifier(hidden_layer_sizes=(size,), max_iter=500))\n",
    "    ])\n",
    "    model_tfidf.fit(X_train, y_train)\n",
    "    predict_train = model_tfidf.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, predict_train)\n",
    "    \n",
    "    predict_valid = model_tfidf.predict(X_valid)\n",
    "    valid_accuracy = accuracy_score(y_valid, predict_valid)\n",
    "    return model_tfidf, train_accuracy, valid_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explore with 100, 200, and 500 hidden layers on the validation set. We set our maximum number of iterations relatively high at 500 epochs because the training sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teodorastereciu/opt/miniconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/teodorastereciu/opt/miniconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_tfidf100, train_accuracy100, valid_accuracy100 = define_model_tfidf(100)\n",
    "model_tfidf200, train_accuracy200, valid_accuracy200 = define_model_tfidf(200)\n",
    "#model_tfidf500, train_accuracy500, valid_accuracy500 = define_model_tfidf(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model100: 0.91355 0.6918\n",
      "model200: 0.998525 0.681\n"
     ]
    }
   ],
   "source": [
    "print(\"model100:\", train_accuracy100, valid_accuracy100)\n",
    "print(\"model200:\", train_accuracy200, valid_accuracy200)\n",
    "#print(\"model500:\", train_accuracy500, valid_accuracy500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the final test, we'll be using ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to Word2Vec, we need to tokenize the training corpus back into a list. The settings we used for the Word2Vec model are not too different from the standard. For a high level explanation of our goal this section, our neural network will take as input Word2Vec embeddings, which use context to predict how likely it is that a word fits with others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize for word2vec\n",
    "X_train_list = [[word for word in line.split()] for line in X_train]\n",
    "# initialize the word2vec model\n",
    "w2v_model = Word2Vec(X_train_list,\n",
    "                    vector_size=100,\n",
    "                    window=5,\n",
    "                    min_count=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our own vectorizer that uses the Word2Vec model above on clean text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_vectorizer(X):\n",
    "    # tokenize\n",
    "    X_list = [[word for word in line.split()] for line in X]\n",
    "    \n",
    "    # average embeddings for each review\n",
    "    X_vect = []\n",
    "    vocab = set(w2v_model.wv.index_to_key)\n",
    "    for line in X_list:\n",
    "        mean_vec = [0]*100\n",
    "        for word in line:\n",
    "            if word in vocab:\n",
    "                mean_vec = np.add(mean_vec, w2v_model.wv[word])\n",
    "        X_vect.append(np.array(mean_vec/len(line)))\n",
    "\n",
    "    return np.array(X_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.44250308 -0.12474672 -0.11875592 ... -0.12294614 -0.01942103\n",
      "   0.0104833 ]\n",
      " [-0.62371868 -0.46918127 -0.41876    ...  0.14540264  0.15781565\n",
      "  -0.41587733]\n",
      " [-0.77591374 -0.39079838 -0.33819434 ...  0.26265447  0.13211061\n",
      "  -0.38795699]\n",
      " ...\n",
      " [-0.30249076 -0.11166037 -0.14121279 ...  0.25851835  0.15269776\n",
      "  -0.36960565]\n",
      " [-0.9231643  -0.41026558 -0.67779348 ...  0.26594043  0.32524223\n",
      "  -0.42071024]\n",
      " [-0.4394797  -0.19827351 -0.24165958 ...  0.09765698  0.37199169\n",
      "  -0.01631636]]\n"
     ]
    }
   ],
   "source": [
    "X_train_vect = w2v_vectorizer(X_train)\n",
    "print(X_train_vect)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We come to finally define our Word2Vec pipeline. In this case, we kept the same hidden layer dimensions as before so we could compare how they perform with similar learning capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model_w2v():\n",
    "    X_train_vect = w2v_vectorizer(X_train)\n",
    "    clf_w2v = MLPClassifier(hidden_layer_sizes=(200,))\n",
    "    clf_w2v.fit(X_train_vect, y_train)\n",
    "    \n",
    "    predict_train = clf_w2v.predict(X_train_vect)\n",
    "    train_accuracy = accuracy_score(y_train, predict_train)\n",
    "\n",
    "    X_valid_vect = w2v_vectorizer(X_valid)\n",
    "    predict_valid = clf_w2v.predict(X_valid_vect)\n",
    "    valid_accuracy = accuracy_score(y_valid, predict_valid)\n",
    "    return clf_w2v, train_accuracy, valid_accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teodorastereciu/opt/miniconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.950875 0.8456\n"
     ]
    }
   ],
   "source": [
    "model_w2v, train_accuracy_w2v, valid_accuracy_w2v = define_model_w2v()\n",
    "print(train_accuracy_w2v, valid_accuracy_w2v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the final test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "265c7b7388b0d02069e08e7a7606766f6a5cc87543dfe585650412d4cca646af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
